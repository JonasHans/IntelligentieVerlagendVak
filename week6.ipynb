{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment  Practice Text classification with Naive Bayes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook made by   (If not filled in correctly: 0 pts for assignment)\n",
    "\n",
    "__Name(s)__: \n",
    "\n",
    "__Student id(s)__ : \n",
    "\n",
    "### Pledge (taken from [Coursera's Honor Code](https://www.coursera.org/about/terms/honorcode) )\n",
    "\n",
    "\n",
    "\n",
    "Put here a selfie with your photo where you hold a signed paper with the following text: (if this is team work, put two selfies here). The link must be to some place on the web, not to a local file. **Assignments without the selfies will not be graded and receive 0 points.**\n",
    "\n",
    "> My answers to homework, quizzes and exams will be my own work (except for assignments that explicitly permit collaboration).\n",
    "\n",
    ">I will not make solutions to homework, quizzes or exams available to anyone else. This includes both solutions written by me, as well as any official solutions provided by the course staff.\n",
    "\n",
    ">I will not engage in any other activities that will dishonestly improve my results or dishonestly improve/hurt the results of others.\n",
    "\n",
    "<img src='link to your selfie'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Text classification with Naive Bayes  \n",
    "        \n",
    "        \n",
    "        \n",
    "<h3>Abstract</h3>\n",
    "<p>We will do text classification on a collection of Dutch parliamentary questions.\n",
    "    The website <a href=\"https://zoek.officielebekendmakingen.nl/zoeken/parlementaire_documenten\">officielebekendmakingen.nl</a>lets you search in \"kamervragen\".\n",
    "    <!--You can donwload\n",
    "    <a href='http://data.politicalmashup.nl/kamervragen/PoliDocs_Kamervragen.zip'>this zipfile with Kamervragen in XML</a>\n",
    "    to see some of the  data in XML format. \n",
    "    It also contains style sheets to show the XML well in a browser.  \n",
    "-->\n",
    "    The <a href='http://maartenmarx.nl/teaching/zoekmachines/LectureNotes/MySQL/'>MYSQL directory</a> contains an <a href='http://maartenmarx.nl/teaching/zoekmachines/LectureNotes/MySQL/KVR14807.xml'>example   Kamervraag XML file</a> and a file `kvr.csv.gz` with 40K kamervragen in a handy csv format. Note that in your browser you see the result of applying stylesheets. So choose View Source or open it in an editor.</p>\n",
    "\n",
    "<h3>First exploration</h3>\n",
    "\n",
    "See below.\n",
    "\n",
    "<h2>Exercises</h2>\n",
    "\n",
    "<p>We will use the fields in elements of the form <tt> &lt;item attribuut=\"Afkomstig_van\"></tt> as our classes. \n",
    "    These are the ministeries to whom the question is addressed.\n",
    "    An example is \n",
    "    <pre>\n",
    "        &lt;item attribuut=\"Afkomstig_van\">Landbouw, Natuurbeheer en Visserij (LNV)&lt;/item>\n",
    "    </pre>\n",
    "    Note that these labels are <strong>not normalized</strong>, see e.g. the counts below:\n",
    "    <pre>\n",
    "Justitie (JUS)                                                   3219\n",
    "Volksgezondheid, Welzijn en Sport (VWS)                          2630\n",
    "Buitenlandse Zaken (BUZA)                                        1796\n",
    "Verkeer en Waterstaat (VW)                                       1441\n",
    "Justitie                                                         1333\n",
    "Sociale Zaken en Werkgelegenheid (SZW)                           1231\n",
    "Onderwijs, Cultuur en Wetenschappen (OCW)                        1187\n",
    "Volkshuisvesting, Ruimtelijke Ordening en Milieubeheer (VROM)     984\n",
    "FinanciÃ«n (FIN)                                                   960\n",
    "Volksgezondheid, Welzijn en Sport                                 951\n",
    "Economische Zaken (EZ)                                            946\n",
    "Buitenlandse Zaken                                                753\n",
    "Binnenlandse Zaken en Koninkrijksrelaties (BZK)                   725\n",
    "Verkeer en Waterstaat                                             724\n",
    "Defensie (DEF)                                                    646\n",
    "Sociale Zaken en Werkgelegenheid                                  607\n",
    "Landbouw, Natuurbeheer en Visserij (LNV)                          586\n",
    "Volkshuisvesting, Ruimtelijke Ordening en Milieubeheer            554\n",
    "Onderwijs, Cultuur en Wetenschappen                               532\n",
    "Vreemdelingenzaken en Integratie (VI)                             466\n",
    "    </pre>\n",
    "</p>\n",
    "\n",
    "<ol>\n",
    "  <li>Normalize the values for \"ministerie\" and choose 10 ministeries to work with. </li>\n",
    "  <li>Implement the two algorithms in Fig MRS.13.2, using your earlier code for creating term and document frequencies.\n",
    "  It might be easier to use the representation and formula given in MRS section 13.4.1.</li>\n",
    "  <li>On this collection, train NB text classifiers for 10 different classes with enough and interesting data.</li>\n",
    "  <li>Compute for each term and each of your 10 classes its utility for that class using mutual information.</li>\n",
    "  <li>For each class, show the top 10 words as in Figure 13.7 in MRS.</li>\n",
    "  <li>Evaluate your classifiers using Precision, Recall and F1. (\n",
    "       <br/>\n",
    "      Give a table in which you show these values for using the top 10, top 100 terms and all terms, for all of your 10 classes.\n",
    "      Thus do feature selection per class, and use for each class the top n best features for that class. \n",
    "      <br/>\n",
    "  Also show the microaverage(s) for all 10 classes together.\n",
    "  <br/>\n",
    "  If you like you can also present this in a figure like MRS.13.8. \n",
    "  Then compute the F1 measure for the same number of terms as in that figure.</li>\n",
    "<li> You have done the complete implementation by yourself. Congratulations! You can also use `scikit-learn` routines for all of this work. Do that. So follow [this text classification tutorial](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)  and implement the same steps but now with your kamervragen dataset. Also use [mutual information feature selection](http://scikit-learn.org/stable/modules/feature_selection.html) to select the K-best features, and compare the results as before.\n",
    "</li>\n",
    "  <li>Reflect and report briefly about your choices in this process and about the obtained results. Also reflect on the differences between the scikit learn approach and the \"own implementation approach\".</li>\n",
    "</ol>\n",
    "\n",
    "<h3>Training/Testing</h3>\n",
    "<p>It is important that you do not test your classifier using documents that have also been used in training.\n",
    "    So split up your collection in a training set and a test set. A 80%-20% split is reasonable.\n",
    "\n",
    "<br/>\n",
    "    If you have too little data you can use 5 or <a href=\"http://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation\">10-fold cross validation</a>.</p>\n",
    "\n",
    "<!--\n",
    "<h2>Form of presentation</h2>\n",
    "<ul>\n",
    "    <li>Make slides or wikipages and have your system running (this could just be an IPython notebook with a classify function that accepts any string and classifies it.) ~~and be able to accept documents from the web.~~ </li>\n",
    "    <li>Create one or two slides or wikipages for each of the sub exercises listed above.\n",
    "</li>\n",
    "<li>Make it clear in the heading of the slides which sub exercises you talk about.</li>\n",
    "    <li>Show running code with one or two  good examples (a TP of course, but also a FP and an error-analysis is nice to show). </li>\n",
    "</ul>\n",
    "-->\n",
    "\n",
    "<h2>Form of handing in your final product</h2>\n",
    "\n",
    "* An IPython notebook with for each question, a MarkDown cell containing the question, a code cell which solves the question, an output cell with the output, followed by a MarkDown cell with explanation/reflection  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment steps\n",
    "\n",
    "#### 1 Normalize\n",
    "* Everything to lowercase\n",
    "* Remove shorthand, add stemming for shorthand (ez -> economische zaken)\n",
    "* Add one smoothin\n",
    "\n",
    "#### 2 Implementation Tf and Df\n",
    "* Split data into 'test' and 'training'\n",
    "* Create data structure for terms and doc frequencies\n",
    "* Create NB classifier function\n",
    "\n",
    "#### 3 Training NB\n",
    "* Train NB with 10 classes\n",
    "\n",
    "#### 4 Mutual information\n",
    "* Compute the utility for the 10 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import math\n",
    "import csv\n",
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import time\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "_dutchStop= stopwords.words('dutch')\n",
    "_names=['jaar', 'partij','titel','vraag','antwoord','ministerie']\n",
    "_ministeries = ['volksgezondheid', 'binnenlands', 'financ', 'justitie', \n",
    "                'econom', 'buitenlands', 'onderwijs', 'verkeer', 'social', 'landbouw']\n",
    "\n",
    "_index_dataset = {}\n",
    "_classes = {}\n",
    "\n",
    "def info_dataset(infile):\n",
    "    with open(infile, 'r') as csvfile:  # open file\n",
    "        csvreader = csv.reader(csvfile, delimiter='\\t', quotechar='|')\n",
    "        \n",
    "        info = {}\n",
    "        \n",
    "        # Go through all kvr's\n",
    "        for kvr in csvreader:\n",
    "            if len(kvr) in info:\n",
    "                info[len(kvr)] += 1\n",
    "            else:\n",
    "                info[len(kvr)] = 1\n",
    "        \n",
    "        print(info)\n",
    "        \n",
    "def normalize_ministerie(ministerie):\n",
    "    # 10 gekozen, we returnen stam die in ministerie kan voorkomen (kunnen meerdere zijn maar kiezen eerste)\n",
    "    try:\n",
    "        for mini in _ministeries:\n",
    "            if  mini in ministerie:\n",
    "                return mini\n",
    "\n",
    "    except Exception as e:\n",
    "        # bij deze set zijn er een stuk of 20 niet te classificeren\n",
    "#         print('Error normalizeren', e, ministerie)\n",
    "        pass\n",
    "    return 'foutief'\n",
    "        \n",
    "def index_dataset(kvrdf):\n",
    "    for mini in _ministeries:\n",
    "        tic = time.time()\n",
    "        print(\"Starting ministry data load: \",mini)\n",
    "        \n",
    "        # Get all data per ministry\n",
    "        miniRows = kvrdf.loc[kvrdf['ministerie'] == mini]\n",
    "        \n",
    "        # Store frequency of class\n",
    "        _classes[mini] = len(miniRows)\n",
    "        \n",
    "        # Bag all text together\n",
    "        text = '\\n'.join(list(miniRows.vraag) + list(miniRows.antwoord) + list(miniRows.titel) + list(miniRows.partij))\n",
    "        tokenizedText = [w for w in nltk.word_tokenize(text.lower()) if w.isalpha() and not w in set(_dutchStop)]\n",
    "        BoW = Counter(tokenizedText)\n",
    "        \n",
    "        # Store counter\n",
    "        _index_dataset[mini] = BoW\n",
    "        \n",
    "        print('Finished reading ministry data in: ', time.time() - tic)\n",
    "\n",
    "# index_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_classes_probs = {}\n",
    "_terms_probs = {}\n",
    "\n",
    "def naive_bayes_trainer():\n",
    "    \n",
    "    # Calculate class probs\n",
    "    for mini in _classes:\n",
    "        _classes_probs[mini] = _classes[mini]/sum(_classes.values())\n",
    "    \n",
    "    # Create dict with all keys\n",
    "    allTerms = Counter()\n",
    "    for d in _index_dataset.values():\n",
    "        allTerms += d\n",
    "\n",
    "    for term in allTerms:\n",
    "        _terms_probs[term] = {}\n",
    "        for mini in _index_dataset:\n",
    "            if term in _index_dataset[mini]:\n",
    "                tcf = (_index_dataset[mini][term] + 1) / (allTerms[term] + len(allTerms))\n",
    "            else:\n",
    "                tcf = 1 / (allTerms[term] + len(allTerms))\n",
    "                \n",
    "            _terms_probs[term][mini] = tcf\n",
    "        \n",
    "# naive_bayes_trainer()\n",
    "# print(_terms_probs['veiligheid'])\n",
    "_classes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_classifier(text):\n",
    "    tokenizedText = [w for w in nltk.word_tokenize(text.lower()) if w.isalpha() and not w in set(_dutchStop) and w in _terms_probs]\n",
    "    \n",
    "    probs = {}\n",
    "    for mini in _classes_probs:\n",
    "        # Calculate prob of text belonging to this class\n",
    "        prob = 0.0\n",
    "        for term in tokenizedText:\n",
    "            prob += math.log(_terms_probs[term][mini])\n",
    "        \n",
    "        probs[mini] = math.log(_classes_probs[mini]) + prob\n",
    "    \n",
    "    return max(probs, key = probs.get)\n",
    "\n",
    "# naive_bayes_classifier(\"veiligheid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ministry data load:  volksgezondheid\n",
      "Finished reading ministry data in:  40.80630826950073\n",
      "Starting ministry data load:  binnenlands\n",
      "Finished reading ministry data in:  17.789682626724243\n",
      "Starting ministry data load:  financ\n",
      "Finished reading ministry data in:  16.030205249786377\n",
      "Starting ministry data load:  justitie\n",
      "Finished reading ministry data in:  42.95576786994934\n",
      "Starting ministry data load:  econom\n",
      "Finished reading ministry data in:  15.003891468048096\n",
      "Starting ministry data load:  buitenlands\n",
      "Finished reading ministry data in:  23.74016499519348\n",
      "Starting ministry data load:  onderwijs\n",
      "Finished reading ministry data in:  18.952534914016724\n",
      "Starting ministry data load:  verkeer\n",
      "Finished reading ministry data in:  22.907968282699585\n",
      "Starting ministry data load:  social\n",
      "Finished reading ministry data in:  19.720290660858154\n",
      "Starting ministry data load:  landbouw\n",
      "Finished reading ministry data in:  14.388017416000366\n",
      "[[682.   0.   0.  62.   0.   0.   0.   0.   0.   0.]\n",
      " [ 51.   3.   0. 339.   0.   0.   0.   2.   0.   0.]\n",
      " [ 99.   0.   6. 194.   0.   4.   0.   2.   0.   0.]\n",
      " [ 12.   0.   0. 902.   0.   1.   0.   1.   0.   0.]\n",
      " [109.   0.   0. 141.   2.   7.   0.   2.   0.   0.]\n",
      " [ 13.   0.   0. 242.   1. 283.   0.   0.   0.   0.]\n",
      " [161.   0.   0. 200.   0.   0.  62.   0.   0.   0.]\n",
      " [ 63.   0.   0. 179.   0.   0.   0. 168.   0.   0.]\n",
      " [206.   0.   0. 123.   0.   1.   0.   0.  29.   0.]\n",
      " [162.   0.   0. 122.   0.   0.   0.   5.   0.  15.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def naive_bayes_test():\n",
    "    # Read CSV file\n",
    "    totalFile = pd.read_csv('http://maartenmarx.nl/teaching/zoekmachines/LectureNotes/MySQL/KVR1000.csv.gz', \n",
    "                   compression='gzip', sep='\\t', \n",
    "                   index_col=0, names=_names,\n",
    "                      )\n",
    "    # normalize\n",
    "    totalFile.ministerie = totalFile.ministerie.str.lower().apply(normalize_ministerie)\n",
    "    totalFile = totalFile.loc[totalFile['ministerie'] != 'foutief']\n",
    "    \n",
    "    # create mask\n",
    "    msk = np.random.rand(len(totalFile)) < 0.8\n",
    "\n",
    "    # create training and test set\n",
    "    train = totalFile[msk]\n",
    "    test = totalFile[~msk]\n",
    "    \n",
    "    # Index the data\n",
    "    index_dataset(train)\n",
    "    \n",
    "    # Train our naive bayes\n",
    "    naive_bayes_trainer()\n",
    "    \n",
    "    confusionMatrix = np.zeros((10,10))\n",
    "    \n",
    "    # test the data\n",
    "    for index, row in test.iterrows():\n",
    "        text = row['vraag']+row['antwoord']+row['titel']+row['partij']\n",
    "        ministerie = row['ministerie']\n",
    "\n",
    "        resultClass = naive_bayes_classifier(text)\n",
    "        \n",
    "        x = _ministeries.index(ministerie)\n",
    "        y = _ministeries.index(resultClass)\n",
    "        \n",
    "        confusionMatrix[x,y] += 1\n",
    "        \n",
    "    print(confusionMatrix)\n",
    "        \n",
    "    \n",
    "naive_bayes_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showTopWords():\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
