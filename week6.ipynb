{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment  Practice Text classification with Naive Bayes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook made by   (If not filled in correctly: 0 pts for assignment)\n",
    "\n",
    "__Name(s)__: Jonas van Oenen, Steven van Beek\n",
    "\n",
    "__Student id(s)__ : 10670947, 10292527\n",
    "\n",
    "### Pledge (taken from [Coursera's Honor Code](https://www.coursera.org/about/terms/honorcode) )\n",
    "\n",
    "<img src='https://imgur.com/FZ1EJGU'/>\n",
    "<img src='https://imgur.com/iKMnTpf'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Text classification with Naive Bayes  \n",
    "        \n",
    "<ol>\n",
    "  <li>Normalize the values for \"ministerie\" and choose 10 ministeries to work with. </li>\n",
    "  <li>Implement the two algorithms in Fig MRS.13.2, using your earlier code for creating term and document frequencies.\n",
    "  It might be easier to use the representation and formula given in MRS section 13.4.1.</li>\n",
    "  <li>On this collection, train NB text classifiers for 10 different classes with enough and interesting data.</li>\n",
    "  <li>Compute for each term and each of your 10 classes its utility for that class using mutual information.</li>\n",
    "  <li>For each class, show the top 10 words as in Figure 13.7 in MRS.</li>\n",
    "  <li>Evaluate your classifiers using Precision, Recall and F1. (\n",
    "       <br/>\n",
    "      Give a table in which you show these values for using the top 10, top 100 terms and all terms, for all of your 10 classes.\n",
    "      Thus do feature selection per class, and use for each class the top n best features for that class. \n",
    "      <br/>\n",
    "  Also show the microaverage(s) for all 10 classes together.\n",
    "  <br/>\n",
    "  If you like you can also present this in a figure like MRS.13.8. \n",
    "  Then compute the F1 measure for the same number of terms as in that figure.</li>\n",
    "<li> You have done the complete implementation by yourself. Congratulations! You can also use `scikit-learn` routines for all of this work. Do that. So follow [this text classification tutorial](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)  and implement the same steps but now with your kamervragen dataset. Also use [mutual information feature selection](http://scikit-learn.org/stable/modules/feature_selection.html) to select the K-best features, and compare the results as before.\n",
    "</li>\n",
    "  <li>Reflect and report briefly about your choices in this process and about the obtained results. Also reflect on the differences between the scikit learn approach and the \"own implementation approach\".</li>\n",
    "</ol>\n",
    "\n",
    "<h3>Training/Testing</h3>\n",
    "<p>It is important that you do not test your classifier using documents that have also been used in training.\n",
    "    So split up your collection in a training set and a test set. A 80%-20% split is reasonable.\n",
    "\n",
    "<br/>\n",
    "    If you have too little data you can use 5 or <a href=\"http://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation\">10-fold cross validation</a>.</p>\n",
    "\n",
    "<!--\n",
    "<h2>Form of presentation</h2>\n",
    "<ul>\n",
    "    <li>Make slides or wikipages and have your system running (this could just be an IPython notebook with a classify function that accepts any string and classifies it.) ~~and be able to accept documents from the web.~~ </li>\n",
    "    <li>Create one or two slides or wikipages for each of the sub exercises listed above.\n",
    "</li>\n",
    "<li>Make it clear in the heading of the slides which sub exercises you talk about.</li>\n",
    "    <li>Show running code with one or two  good examples (a TP of course, but also a FP and an error-analysis is nice to show). </li>\n",
    "</ul>\n",
    "-->\n",
    "\n",
    "<h2>Form of handing in your final product</h2>\n",
    "\n",
    "* An IPython notebook with for each question, a MarkDown cell containing the question, a code cell which solves the question, an output cell with the output, followed by a MarkDown cell with explanation/reflection  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Normalizing and choosing 10 ministeries\n",
    "\n",
    "This first code block imports all  needed libraries and defines the 10 chosen ministeries. Throughout the notebook we will use some static variables indicated by a preceding \"_\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import math\n",
    "import csv\n",
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import time\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "\n",
    "_ministeries = ['volksgezondheid', 'binnenlands', 'financ', 'justitie', \n",
    "                'econom', 'buitenlands', 'onderwijs', 'verkeer', 'social', 'landbouw']\n",
    "_csvFile = 'http://maartenmarx.nl/teaching/zoekmachines/LectureNotes/MySQL/KVR1000.csv.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Term and document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statics for the stop words which we filter\n",
    "_dutchStop= stopwords.words('dutch')\n",
    "\n",
    "# Names for the csv file\n",
    "_names=['jaar', 'partij','titel','vraag','antwoord','ministerie']\n",
    "\n",
    "# Index of our term freq\n",
    "_index_dataset = {}\n",
    "\n",
    "# Doc frequencies\n",
    "_doc_frequencies = {}\n",
    "\n",
    "# Docs and their class\n",
    "_class_docs = {}\n",
    "\n",
    "# Class frequencies\n",
    "_classes = {}\n",
    "        \n",
    "# Function which checks if a ministery is in our chosen set\n",
    "def normalize_ministerie(ministerie):\n",
    "    try:\n",
    "        for mini in _ministeries:\n",
    "            if  mini in ministerie:\n",
    "                return mini\n",
    "\n",
    "    except Exception as e:\n",
    "        # there are some ministries which cannot be classified and we catch the exception here\n",
    "        pass\n",
    "    return 'foutief'\n",
    "        \n",
    "# Function which indexes our dataset and calculates term and document frequency\n",
    "def index_dataset(kvrdf):\n",
    "    for mini in _ministeries:\n",
    "        tic = time.time()\n",
    "        \n",
    "        # Get all data per ministry\n",
    "        miniRows = kvrdf.loc[kvrdf['ministerie'] == mini]\n",
    "        \n",
    "        # Calculate doc frequency\n",
    "        for index, row in miniRows.iterrows():\n",
    "            index = index.strip()\n",
    "            # Set the class of doc\n",
    "            _class_docs[index] = mini\n",
    "            \n",
    "            # Go through text\n",
    "            text = row.vraag + row.antwoord + row.titel + row.partij\n",
    "            tokenizedText = [w for w in nltk.word_tokenize(text.lower()) if w.isalpha() and not w in set(_dutchStop)]\n",
    "            BoW = Counter(tokenizedText)\n",
    "                \n",
    "            for term in BoW:\n",
    "                if not(term in _doc_frequencies):\n",
    "                    _doc_frequencies[term] = {}\n",
    "                \n",
    "                # Set doc freq\n",
    "                _doc_frequencies[term][index] = BoW[term]        \n",
    "        \n",
    "        # Store frequency of class\n",
    "        _classes[mini] = len(miniRows)\n",
    "        \n",
    "        # Bag all text together\n",
    "        text = '\\n'.join(list(miniRows.vraag) + list(miniRows.antwoord) + list(miniRows.titel) + list(miniRows.partij))\n",
    "        \n",
    "        # Tokenize the text, ignoring stopwords and alpha numericals\n",
    "        tokenizedText = [w for w in nltk.word_tokenize(text.lower()) if w.isalpha() and not w in set(_dutchStop)]\n",
    "        \n",
    "        # Create a counter of all terms in the text\n",
    "        BoW = Counter(tokenizedText)\n",
    "        \n",
    "        # Store counter\n",
    "        _index_dataset[mini] = BoW\n",
    "        \n",
    "        print('Finished indexing '+mini+' ministry data in: ', time.time() - tic)\n",
    "        \n",
    "    # Add one smoothing\n",
    "#     for term in _doc_frequencies:\n",
    "#         for doc in _class_docs:\n",
    "#             if not(doc in _doc_frequencies[term]):\n",
    "#                 _doc_frequencies[term][doc] = 1\n",
    "#             else:\n",
    "#                 _doc_frequencies[term][doc] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Naive Bayes\n",
    "\n",
    "This code block contains 3 functions. A training function which calculates probabilities for our training set. A classifier function which runs a piece of text against our training set and classifies it as a specific class. The last function is a test function which splits the data in a training and test set, runs the classifier and prints a confusion matrix of the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Naive Bayes classifier with csv file:  http://maartenmarx.nl/teaching/zoekmachines/LectureNotes/MySQL/KVR1000.csv.gz\n",
      "Finished indexing volksgezondheid ministry data in:  1.6708908081054688\n",
      "Finished indexing binnenlands ministry data in:  0.519162654876709\n",
      "Finished indexing financ ministry data in:  0.4095296859741211\n",
      "Finished indexing justitie ministry data in:  1.3106296062469482\n",
      "Finished indexing econom ministry data in:  0.42391324043273926\n",
      "Finished indexing buitenlands ministry data in:  0.811323881149292\n",
      "Finished indexing onderwijs ministry data in:  0.6373403072357178\n",
      "Finished indexing verkeer ministry data in:  0.879521369934082\n",
      "Finished indexing social ministry data in:  0.8136746883392334\n",
      "Finished indexing landbouw ministry data in:  0.5060243606567383\n",
      "\n",
      "Confusion matrix for Naive Bayes Classifier\n",
      "[[29.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 3.  0.  0. 11.  0.  0.  0.  0.  0.  0.]\n",
      " [ 6.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 2.  0.  0. 32.  0.  0.  0.  0.  0.  0.]\n",
      " [ 9.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 4.  0.  0.  9.  0.  8.  0.  0.  0.  0.]\n",
      " [13.  0.  0.  3.  0.  0.  1.  0.  0.  0.]\n",
      " [ 4.  0.  0.  3.  0.  0.  0.  4.  0.  0.]\n",
      " [ 9.  0.  0.  2.  0.  0.  0.  0.  3.  0.]\n",
      " [ 5.  0.  0.  2.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# Probabilities for classes\n",
    "_classes_probs = {}\n",
    "\n",
    "# Probabilities for terms\n",
    "_terms_probs = {}\n",
    "\n",
    "\n",
    "# Naive bayes trainer which calculates probabilites on our indexed set\n",
    "def naive_bayes_trainer():\n",
    "    \n",
    "    # Calculate prior clas probabilities\n",
    "    for mini in _classes:\n",
    "        _classes_probs[mini] = _classes[mini]/sum(_classes.values())\n",
    "    \n",
    "    # Create dict with term keys of all ministeries and the total occurences\n",
    "    allTerms = Counter()\n",
    "    for d in _index_dataset.values():\n",
    "        allTerms += d\n",
    "\n",
    "    # Go through all terms\n",
    "    for term in allTerms:\n",
    "        _terms_probs[term] = {}\n",
    "        \n",
    "        # Calculate the probability for the term for each class\n",
    "        for mini in _index_dataset:\n",
    "            if term in _index_dataset[mini]:\n",
    "                tcf = (_index_dataset[mini][term] + 1) / (allTerms[term] + len(allTerms))\n",
    "            else:\n",
    "                tcf = 1 / (allTerms[term] + len(allTerms))\n",
    "                \n",
    "            _terms_probs[term][mini] = tcf\n",
    "            \n",
    "# Function which classifies a text based on our naive bayes trained set\n",
    "def naive_bayes_classifier(text):\n",
    "    # Tokenize the text exactly the same as in training\n",
    "    tokenizedText = [w for w in nltk.word_tokenize(text.lower()) if w.isalpha() and not w in set(_dutchStop) and w in _terms_probs]\n",
    "    \n",
    "    # Calculate the probabililty of the text belonging to a class for all classes\n",
    "    probs = {}\n",
    "    for mini in _classes_probs:\n",
    "        # Calculate prob of text belonging to this class\n",
    "        prob = 0.0\n",
    "        for term in tokenizedText:\n",
    "            prob += math.log(_terms_probs[term][mini])\n",
    "        \n",
    "        probs[mini] = math.log(_classes_probs[mini]) + prob\n",
    "    \n",
    "    # Return the arg max of our probabilities\n",
    "    return max(probs, key = probs.get)\n",
    "\n",
    "# Function which runs a test on our naive bayes\n",
    "def naive_bayes_test():\n",
    "    print('Running Naive Bayes classifier with csv file: ', _csvFile)\n",
    "    \n",
    "    # Read CSV file\n",
    "    totalFile = pd.read_csv(_csvFile, \n",
    "                   compression='gzip', sep='\\t', \n",
    "                   index_col=0, names=_names,\n",
    "                      )\n",
    "    # normalize\n",
    "    totalFile.ministerie = totalFile.ministerie.str.lower().apply(normalize_ministerie)\n",
    "    totalFile = totalFile.loc[totalFile['ministerie'] != 'foutief']\n",
    "    \n",
    "    # create mask\n",
    "    msk = np.random.rand(len(totalFile)) < 0.8\n",
    "\n",
    "    # create training and test set\n",
    "    train = totalFile[msk]\n",
    "    test = totalFile[~msk]\n",
    "    \n",
    "    # Index the data\n",
    "    index_dataset(train)\n",
    "    \n",
    "    # Train our naive bayes\n",
    "    naive_bayes_trainer()\n",
    "    \n",
    "    # Create a confusion matrix\n",
    "    confusionMatrix = np.zeros((10,10))\n",
    "    \n",
    "    # Run the test set on our training data\n",
    "    for index, row in test.iterrows():\n",
    "        text = row['vraag']+row['antwoord']+row['titel']+row['partij']\n",
    "        ministerie = row['ministerie']\n",
    "\n",
    "        resultClass = naive_bayes_classifier(text)\n",
    "        \n",
    "        x = _ministeries.index(ministerie)\n",
    "        y = _ministeries.index(resultClass)\n",
    "        \n",
    "        confusionMatrix[x,y] += 1\n",
    "        \n",
    "    \n",
    "    print('')\n",
    "    print('Confusion matrix for Naive Bayes Classifier')\n",
    "    print(confusionMatrix)\n",
    "        \n",
    "    \n",
    "naive_bayes_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Mutual information\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "_mutualInformation = {'volksgezondheid' : {}, 'binnenlands' :{}, 'financ':{}, 'justitie':{}, \n",
    "                'econom':{}, 'buitenlands':{}, 'onderwijs':{}, 'verkeer':{}, 'social':{}, 'landbouw':{}}\n",
    "\n",
    "def mutualInformation():\n",
    "    for term in _terms_probs:\n",
    "        for mini in _ministeries:\n",
    "            # Get relevant and non relevant docs\n",
    "            relevantDocs = [doc for doc in _class_docs if _class_docs[doc] == mini]\n",
    "            nonRelevantDocs = [doc for doc in _class_docs if _class_docs[doc] != mini]\n",
    "            \n",
    "            # Calculate N options\n",
    "            N10 = sum([1 for doc in nonRelevantDocs if doc in _doc_frequencies[term]])\n",
    "            N01 = sum([1 for doc in relevantDocs if not(doc in _doc_frequencies[term])])\n",
    "            N00 = sum([1 for doc in nonRelevantDocs if not(doc in _doc_frequencies[term])])\n",
    "            N11 = sum([1 for doc in relevantDocs if doc in _doc_frequencies[term]])\n",
    "            N = N10+N01+N00+N11\n",
    "            \n",
    "            if (N00 == 0):\n",
    "                N00 = 1\n",
    "            elif (N11 == 0):\n",
    "                N11 = 1\n",
    "            elif (N01 == 0):\n",
    "                N01 = 1\n",
    "            elif (N10 == 0):\n",
    "                N10 = 1\n",
    "            \n",
    "            # Calculate the 4 parts\n",
    "            v1 = (N11/N)*math.log((N*N11)/((N10+N11)*(N01+N11)))\n",
    "            v2 = (N01/N)*math.log((N*N01)/((N00+N01)*(N01+N11)))\n",
    "            v3 = (N10/N)*math.log((N*N10)/((N10+N11)*(N10+N00)))\n",
    "            v4 = (N00/N)*math.log((N*N00)/((N00+N01)*(N10+N00)))\n",
    "            \n",
    "            # Add it all together\n",
    "            mutualResult = v1+v2+v3+v4\n",
    "            \n",
    "            # Add to dict of all mutual information\n",
    "            _mutualInformation[mini][term] = mutualResult\n",
    "    \n",
    "mutualInformation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binnenlands\n",
      "[('behandelen', -0.0014666074197096355), ('uitzetting', -0.0014666074197096355), ('jonge', -0.0014666074197096355), ('patiënt', -0.0014666074197096355), ('utrecht', -0.0014666074197096355), ('gebeurd', -0.0014666074197096355), ('milieubeheer', -0.0014666074197096355), ('rede', -0.0014666074197096355), ('procent', -0.0014666074197096355)]\n",
      "volksgezondheid\n",
      "[('stcrt', -0.0014625419431157705), ('vliegtuigen', -0.0014625419431157705), ('strenge', -0.0014625419431157705), ('geldige', -0.0014625419431157705), ('gerelateerde', -0.0014625419431157705), ('onrechtmatig', -0.0014625419431157705), ('verbinding', -0.0014625419431157705), ('elf', -0.0014625419431157705), ('verontreinigde', -0.0014625419431157705)]\n",
      "buitenlands\n",
      "[('delicten', -0.0014649722379128133), ('komend', -0.0014649722379128133), ('tevoren', -0.0014649722379128133), ('verzorgingshuizen', -0.0014649722379128133), ('jeugdzorg', -0.0014649722379128133), ('helemaal', -0.0014649722379128133), ('berekeningen', -0.0014649722379128133), ('samenspraak', -0.0014649722379128133), ('categorieën', -0.0014649722379128133)]\n",
      "verkeer\n",
      "[('voortbestaan', -0.001467333718822341), ('gebaseerde', -0.001467333718822341), ('onnodig', -0.001467333718822341), ('aanvaard', -0.001467333718822341), ('mishandeling', -0.001467333718822341), ('wao', -0.001467333718822341), ('wetenschappelijke', -0.001467333718822341), ('lichten', -0.001467333718822341), ('verzekeraar', -0.001467333718822341)]\n",
      "onderwijs\n",
      "[('ex', -0.0014670585712848153), ('preventief', -0.0014670585712848153), ('onderling', -0.0014670585712848153), ('bemiddeling', -0.0014670585712848153), ('bewijs', -0.0014670585712848153), ('rekenen', -0.0014670585712848153), ('vergunningen', -0.0014670585712848153), ('concurrentie', -0.0014670585712848153), ('concept', -0.0014670585712848153)]\n",
      "financ\n",
      "[('strafrecht', -0.0014673484121626081), ('fase', -0.0014673484121626081), ('onjuiste', -0.0014673484121626081), ('comité', -0.0014673484121626081), ('hoofd', -0.0014673484121626081), ('uitgevoerde', -0.0014673484121626081), ('voormalige', -0.0014673484121626081), ('evenmin', -0.0014673484121626081), ('speciaal', -0.0014673484121626081)]\n",
      "social\n",
      "[('navo', -0.001466111057553706), ('ernaar', -0.001466111057553706), ('miljoenen', -0.001466111057553706), ('communautaire', -0.001466111057553706), ('facto', -0.001466111057553706), ('tevoren', -0.001466111057553706), ('verzorgingshuizen', -0.001466111057553706), ('jeugdzorg', -0.001466111057553706), ('praktisch', -0.001466111057553706)]\n",
      "econom\n",
      "[('beperking', -0.0014668482179380396), ('verdachte', -0.0014668482179380396), ('grootste', -0.0014668482179380396), ('voorkomende', -0.0014668482179380396), ('verzekering', -0.0014668482179380396), ('gedrag', -0.0014668482179380396), ('ogen', -0.0014668482179380396), ('gekeken', -0.0014668482179380396), ('sturen', -0.0014668482179380396)]\n",
      "justitie\n",
      "[('brabants', -0.0014651405922043775), ('minstens', -0.0014651405922043775), ('arbodienst', -0.0014651405922043775), ('teletekst', -0.0014651405922043775), ('meet', -0.0014651405922043775), ('gewelddadigheden', -0.0014651405922043775), ('attent', -0.0014651405922043775), ('nazorg', -0.0014651405922043775), ('t', -0.0014651405922043775)]\n",
      "landbouw\n",
      "[('rechterlijke', -0.0014666074197096355), ('uitzetting', -0.0014666074197096355), ('meldingen', -0.0014666074197096355), ('bereidheid', -0.0014666074197096355), ('slachtoffers', -0.0014666074197096355), ('pakket', -0.0014666074197096355), ('opdat', -0.0014666074197096355), ('patiënt', -0.0014666074197096355), ('utrecht', -0.0014666074197096355)]\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "def showTopWords():\n",
    "    for mini in _mutualInformation:\n",
    "        sorted_terms = sorted(_mutualInformation[mini].items(), key=operator.itemgetter(1))\n",
    "        print(mini)\n",
    "        print(sorted_terms[1:10])\n",
    "\n",
    "showTopWords()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
